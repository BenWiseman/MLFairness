{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Fairness in Machine Learning\n",
    "\n",
    "In this notebook, we'll explore fairness metrics, model explanation techniques, and bias mitigation approaches. \n",
    "\n",
    "We will:\n",
    "\n",
    "- Generate synthetic data with a sensitive attribute.\n",
    "- Train a logistic regression model.\n",
    "- Evaluate fairness metrics using Fairlearn.\n",
    "- Demonstrate counterfactual fairness.\n",
    "- Explain model decisions with SHAP and LIME.\n",
    "- Mitigate bias using reweighting and adversarial debiasing (via AIF360).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Import Libraries\n",
    "\n",
    "Let's import the required libraries. Make sure you have installed these packages:\n",
    "\n",
    "```bash\n",
    "pip install numpy pandas scikit-learn fairlearn aif360 'aif360[inFairness]' shap lime tensorflow \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fairness metrics from Fairlearn\n",
    "from fairlearn.metrics import MetricFrame, selection_rate, true_positive_rate, false_positive_rate\n",
    "\n",
    "# Model explanation libraries\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Bias mitigation using AIF360\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
    "\n",
    "# TensorFlow for adversarial debiasing\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Data\n",
    "\n",
    "We create a binary classification dataset with 5 features. Additionally, we add a sensitive attribute (0 or 1) to simulate a protected group. This attribute will help us later to check if our model is biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat1</th>\n",
       "      <th>feat2</th>\n",
       "      <th>feat3</th>\n",
       "      <th>feat4</th>\n",
       "      <th>feat5</th>\n",
       "      <th>sensitive</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>0.558709</td>\n",
       "      <td>-0.304063</td>\n",
       "      <td>-0.675708</td>\n",
       "      <td>-0.327199</td>\n",
       "      <td>0.144969</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-0.000163</td>\n",
       "      <td>1.021126</td>\n",
       "      <td>-1.087246</td>\n",
       "      <td>0.484538</td>\n",
       "      <td>-2.493911</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>-0.853323</td>\n",
       "      <td>0.035071</td>\n",
       "      <td>1.532368</td>\n",
       "      <td>0.296036</td>\n",
       "      <td>0.827214</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>-0.979093</td>\n",
       "      <td>0.785848</td>\n",
       "      <td>-0.617652</td>\n",
       "      <td>0.693431</td>\n",
       "      <td>-0.872002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>-1.325967</td>\n",
       "      <td>1.051464</td>\n",
       "      <td>-0.317715</td>\n",
       "      <td>0.933029</td>\n",
       "      <td>-1.149683</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feat1     feat2     feat3     feat4     feat5  sensitive  target\n",
       "541  0.558709 -0.304063 -0.675708 -0.327199  0.144969          1       1\n",
       "440 -0.000163  1.021126 -1.087246  0.484538 -2.493911          1       1\n",
       "482 -0.853323  0.035071  1.532368  0.296036  0.827214          0       0\n",
       "422 -0.979093  0.785848 -0.617652  0.693431 -0.872002          0       0\n",
       "778 -1.325967  1.051464 -0.317715  0.933029 -1.149683          1       0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create synthetic binary classification data\n",
    "X, y = make_classification(n_samples=1000, n_features=5, random_state=42)\n",
    "\n",
    "# Create a sensitive attribute (0 or 1) with some added noise\n",
    "sensitive = (y + np.random.binomial(1, 0.3, size=y.shape)).clip(0, 1)\n",
    "\n",
    "# Build a DataFrame with feature columns, sensitive attribute, and target\n",
    "df = pd.DataFrame(X, columns=[f'feat{i}' for i in range(1, 6)])\n",
    "df['sensitive'] = sensitive\n",
    "df['target'] = y\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train, test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Take a peek at the training data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train a Logistic Regression Model\n",
    "\n",
    "We now train a logistic regression model using only the non-sensitive features. This simulates a situation where the sensitive attribute is not part of the model's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Define the feature set (excluding the sensitive attribute)\n",
    "features = [f'feat{i}' for i in range(1, 6)]\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(train[features], train['target'])\n",
    "\n",
    "# Generate predictions on the test set\n",
    "test = test.copy()  # avoid potential warnings\n",
    "test['pred'] = model.predict(test[features])\n",
    "\n",
    "# Check overall model accuracy\n",
    "acc = accuracy_score(test['target'], test['pred'])\n",
    "print('Overall Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Fairness Metrics with Fairlearn\n",
    "\n",
    "We now use Fairlearn's `MetricFrame` to calculate fairness metrics across groups defined by the sensitive attribute. \n",
    "\n",
    "### Key Metrics:\n",
    "\n",
    "- **Selection Rate (Demographic Parity):** The rate of positive predictions for each group.\n",
    "- **True Positive Rate (TPR) & False Positive Rate (FPR) (Equalized Odds):** The rates of correctly and incorrectly classified positives across groups.\n",
    "- **Disparate Impact:** The ratio of the selection rates (unprivileged/privileged)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness metrics by sensitive group:\n",
      "           accuracy  selection_rate       tpr       fpr\n",
      "sensitive                                              \n",
      "0          0.944954        0.055046  0.000000  0.055046\n",
      "1          0.858639        0.717277  0.857143  0.135135\n",
      "\n",
      "Disparate Impact Ratio (unprivileged/privileged): 0.07674278443715261\n",
      "Equalized Odds differences (TPR diff, FPR diff): 0.8571428571428571 0.08008926357550211\n"
     ]
    }
   ],
   "source": [
    "# Calculate fairness metrics using MetricFrame\n",
    "mf = MetricFrame(metrics={\n",
    "                    'accuracy': accuracy_score,\n",
    "                    'selection_rate': selection_rate,\n",
    "                    'tpr': true_positive_rate,\n",
    "                    'fpr': false_positive_rate},\n",
    "                 y_true=test['target'],\n",
    "                 y_pred=test['pred'],\n",
    "                 sensitive_features=test['sensitive'])\n",
    "\n",
    "print('Fairness metrics by sensitive group:')\n",
    "print(mf.by_group)\n",
    "\n",
    "# Compute Disparate Impact: ratio of selection rates between groups\n",
    "sr_priv = mf.by_group.loc[1, 'selection_rate']\n",
    "sr_unpriv = mf.by_group.loc[0, 'selection_rate']\n",
    "disparate_impact = sr_unpriv / sr_priv if sr_priv != 0 else np.nan\n",
    "print('\\nDisparate Impact Ratio (unprivileged/privileged):', disparate_impact)\n",
    "\n",
    "# Calculate Equalized Odds differences (difference in TPR and FPR)\n",
    "tpr_diff = mf.by_group.loc[1, 'tpr'] - mf.by_group.loc[0, 'tpr']\n",
    "fpr_diff = mf.by_group.loc[1, 'fpr'] - mf.by_group.loc[0, 'fpr']\n",
    "print('Equalized Odds differences (TPR diff, FPR diff):', tpr_diff, fpr_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Counterfactual Fairness\n",
    "\n",
    "Counterfactual fairness asks, \"Would the prediction change if we flip the sensitive attribute?\" \n",
    "\n",
    "Since our initial model does not include the sensitive attribute, flipping it does not change the prediction. \n",
    "\n",
    "For demonstration, we train another model that includes the sensitive attribute and compare its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original prediction (without sensitive attribute): 1\n",
      "\n",
      "Model including sensitive attribute:\n",
      "  Original prediction: 1\n",
      "  Counterfactual prediction after flipping sensitive: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Select a test instance\n",
    "instance = test.iloc[0]\n",
    "orig_pred = model.predict(instance[features].values.reshape(1, -1))[0]\n",
    "print('Original prediction (without sensitive attribute):', orig_pred)\n",
    "\n",
    "# Train a model that includes the sensitive attribute\n",
    "features_with_sensitive = features + ['sensitive']\n",
    "model_with_sensitive = LogisticRegression(solver='lbfgs')\n",
    "model_with_sensitive.fit(train[features_with_sensitive], train['target'])\n",
    "\n",
    "# Get prediction with the sensitive attribute\n",
    "orig_pred_sensitive = model_with_sensitive.predict(instance[features_with_sensitive].values.reshape(1, -1))[0]\n",
    "\n",
    "# Flip the sensitive attribute to see if the prediction changes\n",
    "counterfactual_instance = instance.copy()\n",
    "counterfactual_instance['sensitive'] = 1 - counterfactual_instance['sensitive']\n",
    "cf_pred_sensitive = model_with_sensitive.predict(counterfactual_instance[features_with_sensitive].values.reshape(1, -1))[0]\n",
    "\n",
    "print('\\nModel including sensitive attribute:')\n",
    "print('  Original prediction:', orig_pred_sensitive)\n",
    "print('  Counterfactual prediction after flipping sensitive:', cf_pred_sensitive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Explanations with SHAP and LIME\n",
    "\n",
    "Understanding why a model makes a particular prediction is crucial. \n",
    "\n",
    "**SHAP (SHapley Additive exPlanations):** Uses game theory to assign an importance value to each feature.\n",
    "\n",
    "**LIME (Local Interpretable Model-agnostic Explanations):** Approximates the model locally with a simpler, interpretable model.\n",
    "\n",
    "Let's generate explanations for our chosen instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP values for the chosen instance:\n",
      "[ 1.87910887  0.46366104  0.06430308  0.64512978 -0.79041196]\n",
      "\n",
      "LIME explanation (text output):\n",
      "[('feat1 > 1.05', 0.5417484470738576), ('feat5 > 1.02', -0.18140358227278397), ('feat4 <= -0.53', 0.12640461313989831), ('feat2 <= -0.47', 0.08140704559661191), ('feat3 <= -0.65', 0.01342174639145135)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/shap/explainers/_linear.py:95: FutureWarning: The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, maskers.Partition or maskers.Impute).\n",
      "  warnings.warn(wmsg, FutureWarning)\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# SHAP explanation for our logistic regression model\n",
    "explainer = shap.LinearExplainer(model, train[features], feature_perturbation='interventional')\n",
    "shap_values = explainer.shap_values(instance[features])\n",
    "print('SHAP values for the chosen instance:')\n",
    "print(shap_values)\n",
    "\n",
    "# To visualise the explanation in a notebook, you can run these commands in a separate cell:\n",
    "# shap.initjs()\n",
    "# shap.force_plot(explainer.expected_value, shap_values, instance[features])\n",
    "\n",
    "# LIME explanation\n",
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=train[features].values,\n",
    "    feature_names=features,\n",
    "    class_names=['0', '1'],\n",
    "    mode='classification'\n",
    ")\n",
    "lime_exp = lime_explainer.explain_instance(instance[features].values, model.predict_proba)\n",
    "print('\\nLIME explanation (text output):')\n",
    "print(lime_exp.as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bias Mitigation with Reweighting (AIF360)\n",
    "\n",
    "Reweighting adjusts the importance of samples to balance the representation of the sensitive groups. \n",
    "\n",
    "We'll convert our training data into AIF360's `BinaryLabelDataset`, apply reweighting, and then train a new model using the adjusted sample weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness metrics by group after reweighting:\n",
      "           accuracy  selection_rate       tpr       fpr\n",
      "sensitive                                              \n",
      "0          0.972477        0.027523  0.000000  0.027523\n",
      "1          0.821990        0.680628  0.811688  0.135135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/aif360/algorithms/preprocessing/reweighing.py:68: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  self.w_up_fav = n_fav*n_up / (n*n_up_fav)\n"
     ]
    }
   ],
   "source": [
    "# Convert training data to an AIF360 BinaryLabelDataset\n",
    "train_aif = BinaryLabelDataset(favorable_label=1,\n",
    "                               unfavorable_label=0,\n",
    "                               df=train.copy(),\n",
    "                               label_names=['target'],\n",
    "                               protected_attribute_names=['sensitive'])\n",
    "\n",
    "# Apply reweighting to balance the sensitive groups\n",
    "RW = Reweighing(unprivileged_groups=[{'sensitive': 0}],\n",
    "                privileged_groups=[{'sensitive': 1}])\n",
    "train_aif_transf = RW.fit_transform(train_aif)\n",
    "sample_weights = train_aif_transf.instance_weights  # these are the adjusted sample weights\n",
    "\n",
    "# Train a new logistic regression model using the reweighted samples\n",
    "model_rw = LogisticRegression(solver='lbfgs')\n",
    "model_rw.fit(train[features], train['target'], sample_weight=sample_weights)\n",
    "test['pred_rw'] = model_rw.predict(test[features])\n",
    "\n",
    "# Evaluate fairness metrics for the reweighted model\n",
    "mf_rw = MetricFrame(metrics={\n",
    "                        'accuracy': accuracy_score,\n",
    "                        'selection_rate': selection_rate,\n",
    "                        'tpr': true_positive_rate,\n",
    "                        'fpr': false_positive_rate},\n",
    "                    y_true=test['target'],\n",
    "                    y_pred=test['pred_rw'],\n",
    "                    sensitive_features=test['sensitive'])\n",
    "print('Fairness metrics by group after reweighting:')\n",
    "print(mf_rw.by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bias Mitigation with Adversarial Debiasing (AIF360)\n",
    "\n",
    "Adversarial Debiasing trains a model while an adversary simultaneously attempts to predict the sensitive attribute. This forces the model to ignore sensitive information. \n",
    "\n",
    "We need to set up a TensorFlow session to run this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.672155; batch adversarial loss: 0.741866\n",
      "epoch 1; iter: 0; batch classifier loss: 0.615038; batch adversarial loss: 0.741554\n",
      "epoch 2; iter: 0; batch classifier loss: 0.555537; batch adversarial loss: 0.759447\n",
      "epoch 3; iter: 0; batch classifier loss: 0.504566; batch adversarial loss: 0.758306\n",
      "epoch 4; iter: 0; batch classifier loss: 0.508343; batch adversarial loss: 0.758476\n",
      "epoch 5; iter: 0; batch classifier loss: 0.446736; batch adversarial loss: 0.749879\n",
      "epoch 6; iter: 0; batch classifier loss: 0.455645; batch adversarial loss: 0.757581\n",
      "epoch 7; iter: 0; batch classifier loss: 0.502484; batch adversarial loss: 0.745647\n",
      "epoch 8; iter: 0; batch classifier loss: 0.482942; batch adversarial loss: 0.766185\n",
      "epoch 9; iter: 0; batch classifier loss: 0.445025; batch adversarial loss: 0.740793\n",
      "epoch 10; iter: 0; batch classifier loss: 0.456150; batch adversarial loss: 0.748575\n",
      "epoch 11; iter: 0; batch classifier loss: 0.416245; batch adversarial loss: 0.751893\n",
      "epoch 12; iter: 0; batch classifier loss: 0.469941; batch adversarial loss: 0.752675\n",
      "epoch 13; iter: 0; batch classifier loss: 0.403633; batch adversarial loss: 0.722591\n",
      "epoch 14; iter: 0; batch classifier loss: 0.458757; batch adversarial loss: 0.735443\n",
      "epoch 15; iter: 0; batch classifier loss: 0.454594; batch adversarial loss: 0.748006\n",
      "epoch 16; iter: 0; batch classifier loss: 0.347032; batch adversarial loss: 0.700059\n",
      "epoch 17; iter: 0; batch classifier loss: 0.424765; batch adversarial loss: 0.724847\n",
      "epoch 18; iter: 0; batch classifier loss: 0.386420; batch adversarial loss: 0.708428\n",
      "epoch 19; iter: 0; batch classifier loss: 0.399434; batch adversarial loss: 0.721668\n",
      "epoch 20; iter: 0; batch classifier loss: 0.430551; batch adversarial loss: 0.737263\n",
      "epoch 21; iter: 0; batch classifier loss: 0.417398; batch adversarial loss: 0.730968\n",
      "epoch 22; iter: 0; batch classifier loss: 0.393412; batch adversarial loss: 0.712890\n",
      "epoch 23; iter: 0; batch classifier loss: 0.354750; batch adversarial loss: 0.712035\n",
      "epoch 24; iter: 0; batch classifier loss: 0.454840; batch adversarial loss: 0.691081\n",
      "epoch 25; iter: 0; batch classifier loss: 0.375077; batch adversarial loss: 0.709521\n",
      "epoch 26; iter: 0; batch classifier loss: 0.343928; batch adversarial loss: 0.687540\n",
      "epoch 27; iter: 0; batch classifier loss: 0.423580; batch adversarial loss: 0.718670\n",
      "epoch 28; iter: 0; batch classifier loss: 0.360232; batch adversarial loss: 0.677063\n",
      "epoch 29; iter: 0; batch classifier loss: 0.413664; batch adversarial loss: 0.694633\n",
      "epoch 30; iter: 0; batch classifier loss: 0.370086; batch adversarial loss: 0.691675\n",
      "epoch 31; iter: 0; batch classifier loss: 0.432658; batch adversarial loss: 0.684531\n",
      "epoch 32; iter: 0; batch classifier loss: 0.439125; batch adversarial loss: 0.678634\n",
      "epoch 33; iter: 0; batch classifier loss: 0.414657; batch adversarial loss: 0.710366\n",
      "epoch 34; iter: 0; batch classifier loss: 0.329122; batch adversarial loss: 0.678445\n",
      "epoch 35; iter: 0; batch classifier loss: 0.420425; batch adversarial loss: 0.705775\n",
      "epoch 36; iter: 0; batch classifier loss: 0.463827; batch adversarial loss: 0.691658\n",
      "epoch 37; iter: 0; batch classifier loss: 0.455294; batch adversarial loss: 0.694823\n",
      "epoch 38; iter: 0; batch classifier loss: 0.415668; batch adversarial loss: 0.660246\n",
      "epoch 39; iter: 0; batch classifier loss: 0.428656; batch adversarial loss: 0.689873\n",
      "epoch 40; iter: 0; batch classifier loss: 0.508453; batch adversarial loss: 0.714973\n",
      "epoch 41; iter: 0; batch classifier loss: 0.378440; batch adversarial loss: 0.715789\n",
      "epoch 42; iter: 0; batch classifier loss: 0.459984; batch adversarial loss: 0.673647\n",
      "epoch 43; iter: 0; batch classifier loss: 0.390784; batch adversarial loss: 0.641952\n",
      "epoch 44; iter: 0; batch classifier loss: 0.503615; batch adversarial loss: 0.724491\n",
      "epoch 45; iter: 0; batch classifier loss: 0.471833; batch adversarial loss: 0.695450\n",
      "epoch 46; iter: 0; batch classifier loss: 0.330875; batch adversarial loss: 0.663143\n",
      "epoch 47; iter: 0; batch classifier loss: 0.465982; batch adversarial loss: 0.709666\n",
      "epoch 48; iter: 0; batch classifier loss: 0.463956; batch adversarial loss: 0.706570\n",
      "epoch 49; iter: 0; batch classifier loss: 0.446001; batch adversarial loss: 0.694363\n",
      "Fairness metrics by group after adversarial debiasing:\n",
      "           accuracy  selection_rate       tpr       fpr\n",
      "sensitive                                              \n",
      "0          0.477064        0.522936  0.000000  0.522936\n",
      "1          0.900524        0.821990  0.948052  0.297297\n"
     ]
    }
   ],
   "source": [
    "# Reset the TensorFlow graph to clear any existing variables\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Set up a new TensorFlow session for adversarial debiasing\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "adv_debiasing = AdversarialDebiasing(\n",
    "    privileged_groups=[{'sensitive': 1}],\n",
    "    unprivileged_groups=[{'sensitive': 0}],\n",
    "    scope_name='adv_debiasing',\n",
    "    sess=sess,\n",
    "    num_epochs=50,\n",
    "    debias=True\n",
    ")\n",
    "\n",
    "# Train the adversarial debiased model using the AIF360 training dataset\n",
    "adv_debiasing.fit(train_aif)\n",
    "\n",
    "# Remove extra prediction columns from test before creating AIF360 dataset\n",
    "columns_to_remove = ['pred', 'pred_rw', 'adv_pred']\n",
    "test_clean = test.drop(columns=[col for col in columns_to_remove if col in test.columns], errors='ignore')\n",
    "\n",
    "# Convert cleaned test data to an AIF360 BinaryLabelDataset\n",
    "test_aif = BinaryLabelDataset(\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0,\n",
    "    df=test_clean.copy(),\n",
    "    label_names=['target'],\n",
    "    protected_attribute_names=['sensitive']\n",
    ")\n",
    "\n",
    "# Make predictions with the debiased model\n",
    "test_pred = adv_debiasing.predict(test_aif)\n",
    "test['adv_pred'] = test_pred.labels.ravel()\n",
    "\n",
    "# Evaluate fairness metrics for the adversarial debiased model\n",
    "mf_adv = MetricFrame(\n",
    "    metrics={\n",
    "        'accuracy': accuracy_score,\n",
    "        'selection_rate': selection_rate,\n",
    "        'tpr': true_positive_rate,\n",
    "        'fpr': false_positive_rate\n",
    "    },\n",
    "    y_true=test['target'],\n",
    "    y_pred=test['adv_pred'],\n",
    "    sensitive_features=test['sensitive']\n",
    ")\n",
    "print('Fairness metrics by group after adversarial debiasing:')\n",
    "print(mf_adv.by_group)\n",
    "\n",
    "sess.close()  # Ka pai rā tēnei mahi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this tutorial we:\n",
    "\n",
    "- Generated synthetic data with a sensitive attribute.\n",
    "- Trained a logistic regression model and evaluated its fairness using Fairlearn.\n",
    "- Explored counterfactual fairness by flipping the sensitive attribute.\n",
    "- Explained predictions using SHAP and LIME.\n",
    "- Mitigated bias using reweighting and adversarial debiasing (via AIF360).\n",
    "\n",
    "This workflow can serve as a basis for building fairer machine learning models. Cheers, mate!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
